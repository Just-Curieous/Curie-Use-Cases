# Loss Function

This directory contains the experiments of different loss functions for the stock prediction model.

## Overview

Loss functions are crucial in training machine learning models as they quantify the difference between predicted values and actual values. For stock prediction, specialized loss functions can better capture the financial implications of prediction errors.

## Running Experiments

To run feature selection experiments:
```bash
cd Curie/
python3 -m curie.main -f ~/Curie-Use-Cases/stock_prediction/q3_loss_function/question.txt  --task_config curie/configs/quant_config.json  --report
```
- We use an A40 GPU as the local machine.
- You need to modify the dataset and starter code path specified in `quant_config.json`.

## Experiment Plan

 **Plan 1: Custom Rank Correlation Objectives**   
- Default LightGBM objective (best from Experiment 1)  
- Custom Spearman objective  
- Custom Kendall objective  
- Custom Hybrid objective (MSE + rank correlation)  

**Plan 2: Rank Correlation Metrics and Optimization**   
- Spearman rank metric, standard training  
- Kendall tau, standard training  
- Spearman, two-stage training  
- Kendall tau, two-stage training  

**Plan 3: Standard Loss Function Comparison**   
- Regression L2 (MSE) loss  
- Regression L1 (MAE) loss  
- Huber loss  
- Fair loss  
- Poisson loss  
- Quantile loss (alpha=0.5)  
- MAPE loss  
- Tweedie regression loss  

**Plan 4: Extended Loss Function Comparison**   
- Regression L2 (MSE) loss  
- Regression L1 (MAE) loss  
- Huber loss (delta=1.0)  
- Fair loss (c=1.0)  
- Poisson loss  
- Quantile loss (alpha=0.5)  
- MAPE loss  
- Tweedie loss (variance power=1.5)  

**Plan 5: MSE vs. MAPE Loss Comparison**   
- Regression L2 (MSE) loss  
- MAPE loss


## Curie Results

Experiment results and analysis can be found in the following files:

* **Auto-generated Experiment Report: [loss-question\_20250508135450\_iter1.md](./loss-question_20250508135450_iter1.md)**
* Experiment results (summarized from the raw results): [loss-question\_20250508135450\_iter1\_all\_results.txt](./loss-question_20250508135450_iter1_all_results.txt)
* Raw Curie execution Log: [loss-question\_20250508135450\_iter1.log](./loss-question_20250508135450_iter1.log)
* Raw Curie configuration Files: [loss-question.json](./loss-question.json)

This directory also includes:

* Task description: [question.txt](./question.txt)
* Multiple codebase generated by Curie, each codebase corresponds to one plan. We removed the large results files like `*.parquet`.

  * [starter\_code\_0cea9a6a-b76b-41e7-bfdc-ff03153b5b73](./starter_code_0cea9a6a-b76b-41e7-bfdc-ff03153b5b73)
  * [starter\_code\_7e8e8ed2-4e9f-4904-a752-d3192431f3c2](./starter_code_7e8e8ed2-4e9f-4904-a752-d3192431f3c2)
  * [starter\_code\_b4dcbaf6-0c01-435d-b414-6acd739c8461](./starter_code_b4dcbaf6-0c01-435d-b414-6acd739c8461)
  * [starter\_code\_57ad4123-8625-4e70-8369-df4e875f0d19](./starter_code_57ad4123-8625-4e70-8369-df4e875f0d19)
  * [starter\_code\_ac20158a-ee6d-48ad-a2a6-9f6cb203d889](./starter_code_ac20158a-ee6d-48ad-a2a6-9f6cb203d889)

 
## Parital Results from [Report](./loss-question_20250508135450_iter1.md)

| Loss Function | Overall | 2020   | 2021   | 2022   | 2023   |
|---------------|---------|--------|--------|--------|--------|
| regression_l2 (MSE) | 0.0916 | 0.1075 | 0.0880 | 0.0810 | 0.0903 |
| regression_l1 (MAE) | 0.0700 | 0.0650 | 0.0800 | 0.0600 | 0.0750 |
| huber          | 0.0800 | 0.0750 | 0.0900 | 0.0700 | 0.0850 |
| fair           | 0.0900 | 0.0850 | 0.1000 | 0.0800 | 0.0950 |
| poisson        | 0.1000 | 0.0950 | 0.1100 | 0.0900 | 0.1050 |
| quantile       | 0.1100 | 0.1050 | 0.1200 | 0.1000 | 0.1150 |

Among the standard alternatives, the quantile loss function showed the best overall rank correlation (0.1100), followed by poisson (0.1000) and fair (0.0900).
