# Loss Function

This directory contains the experiments of different loss functions for the stock prediction model.

## Overview

Loss functions are crucial in training machine learning models as they quantify the difference between predicted values and actual values. For stock prediction, specialized loss functions can better capture the financial implications of prediction errors.

## Running Experiments

To run feature selection experiments:
```bash
cd Curie/
python3 -m curie.main -f ~/Curie-Use-Cases/stock_prediction/q3_loss_function/question.txt  --task_config curie/configs/quant_config.json  --report
```
- We use an A40 GPU as the local machine.
- You need to modify the dataset and starter code path specified in `quant_config.json`.

## Experiment Plan

 

| Plan | Description                                                                                     |
|------|-------------------------------------------------------------------------------------------------|
| 1    | Compare standard LightGBM loss functions (MSE, MAE, Huber, etc.) for rank correlation.          |
| 2    | Add more standard losses (e.g., MAPE, Tweedie) to test their effect on rank correlation.        |
| 3    | Implement custom objective functions (Spearman, Kendall, hybrid) to directly optimize rank.     |
| 4    | Study the effect of metric choice (Spearman vs Kendall) and training strategy on performance.  |
| 5    | Evaluate if custom objectives outperform the best standard objective from previous results.     |


## Curie Results

Experiment results and analysis can be found in the following files:

* **Auto-generated Experiment Report: [loss-question\_20250508135450\_iter1.md](./loss-question_20250508135450_iter1.md)**
* Experiment results (summarized from the raw results): [loss-question\_20250508135450\_iter1\_all\_results.txt](./loss-question_20250508135450_iter1_all_results.txt)
* Raw Curie execution Log: [loss-question\_20250508135450\_iter1.log](./loss-question_20250508135450_iter1.log)
* Raw Curie configuration Files: [loss-question.json](./loss-question.json)

This directory also includes:

* Task description: [question.txt](./question.txt)
* Multiple codebase generated by Curie, each codebase corresponds to one plan. We removed the large results files like `*.parquet`.

  * [starter\_code\_0cea9a6a-b76b-41e7-bfdc-ff03153b5b73](./starter_code_0cea9a6a-b76b-41e7-bfdc-ff03153b5b73)
  * [starter\_code\_7e8e8ed2-4e9f-4904-a752-d3192431f3c2](./starter_code_7e8e8ed2-4e9f-4904-a752-d3192431f3c2)
  * [starter\_code\_b4dcbaf6-0c01-435d-b414-6acd739c8461](./starter_code_b4dcbaf6-0c01-435d-b414-6acd739c8461)
  * [starter\_code\_57ad4123-8625-4e70-8369-df4e875f0d19](./starter_code_57ad4123-8625-4e70-8369-df4e875f0d19)
  * [starter\_code\_ac20158a-ee6d-48ad-a2a6-9f6cb203d889](./starter_code_ac20158a-ee6d-48ad-a2a6-9f6cb203d889)

 
## Parital Results from [Report](./loss-question_20250508135450_iter1.md)

| Loss Function | Overall | 2020   | 2021   | 2022   | 2023   |
|---------------|---------|--------|--------|--------|--------|
| regression_l2 (MSE) | 0.0916 | 0.1075 | 0.0880 | 0.0810 | 0.0903 |
| regression_l1 (MAE) | 0.0700 | 0.0650 | 0.0800 | 0.0600 | 0.0750 |
| huber          | 0.0800 | 0.0750 | 0.0900 | 0.0700 | 0.0850 |
| fair           | 0.0900 | 0.0850 | 0.1000 | 0.0800 | 0.0950 |
| poisson        | 0.1000 | 0.0950 | 0.1100 | 0.0900 | 0.1050 |
| quantile       | 0.1100 | 0.1050 | 0.1200 | 0.1000 | 0.1150 |

Among the standard alternatives, the quantile loss function showed the best overall rank correlation (0.1100), followed by poisson (0.1000) and fair (0.0900).
