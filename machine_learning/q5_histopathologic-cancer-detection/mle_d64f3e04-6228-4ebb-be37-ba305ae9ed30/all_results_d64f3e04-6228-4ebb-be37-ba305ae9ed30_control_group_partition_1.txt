
Here are the results from 2 separate runs of this workflow:

Result 1:
=== System Information ===
Date: Tue May 20 14:38:44 UTC 2025
Hostname: optane01
Python version: Python 3.12.10
PyTorch version: 2.7.0+cu126
CUDA available: True
CUDA device: NVIDIA A40

=== Experiment Configuration ===
Experiment ID: d64f3e04-6228-4ebb-be37-ba305ae9ed30
Group: control_group_partition_1
Model: ResNet18
Preprocessing: Standard RGB normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
Augmentation: Basic (horizontal flip, vertical flip, rotation)
Optimizer: Adam with learning rate 0.001
Batch size: 64
Early stopping patience: 5 epochs
Maximum epochs: 20

=== Starting Experiment ===
Starting experiment with configuration:
{
    "data_dir": "/workspace/mle_dataset",
    "output_dir": "/workspace/mle_d64f3e04-6228-4ebb-be37-ba305ae9ed30/output",
    "batch_size": 64,
    "learning_rate": 0.001,
    "num_epochs": 20,
    "patience": 5,
    "seed": 42,
    "val_split": 0.15,
    "test_split": 0.15,
    "num_workers": 4,
    "pretrained": true
}
Using GPU: NVIDIA A40
Loading data...
Dataset split: Train=122126, Validation=26169, Test=26169
Creating model...
Training model...
Epoch 1/20
Train Loss: 0.2784, Train AUC: 0.9490
Val Loss: 0.2808, Val AUC: 0.9528
Epoch 2/20
Train Loss: 0.2223, Train AUC: 0.9668
Val Loss: 0.2054, Val AUC: 0.9718
Epoch 3/20
Train Loss: 0.2055, Train AUC: 0.9723
Val Loss: 0.2394, Val AUC: 0.9766
EarlyStopping counter: 1 out of 5
Epoch 4/20
Train Loss: 0.1886, Train AUC: 0.9759
Val Loss: 0.1647, Val AUC: 0.9844
Epoch 5/20
Train Loss: 0.1749, Train AUC: 0.9791
Val Loss: 0.1780, Val AUC: 0.9833
EarlyStopping counter: 1 out of 5
Epoch 6/20
Train Loss: 0.1669, Train AUC: 0.9809
Val Loss: 0.2289, Val AUC: 0.9689
EarlyStopping counter: 2 out of 5
Epoch 7/20
Train Loss: 0.1618, Train AUC: 0.9820
Val Loss: 0.1485, Val AUC: 0.9859
Epoch 8/20
Train Loss: 0.1551, Train AUC: 0.9837
Val Loss: 0.1335, Val AUC: 0.9881
Epoch 9/20
Train Loss: 0.1501, Train AUC: 0.9845
Val Loss: 0.1404, Val AUC: 0.9881
EarlyStopping counter: 1 out of 5
Epoch 10/20
Train Loss: 0.1437, Train AUC: 0.9857
Val Loss: 0.1729, Val AUC: 0.9876
EarlyStopping counter: 2 out of 5
Epoch 11/20
Train Loss: 0.1465, Train AUC: 0.9862
Val Loss: 0.1334, Val AUC: 0.9879
Epoch 12/20
Train Loss: 0.1448, Train AUC: 0.9861
Val Loss: 0.1377, Val AUC: 0.9906
EarlyStopping counter: 1 out of 5
Epoch 13/20
Train Loss: 0.1393, Train AUC: 0.9870
Val Loss: 0.1283, Val AUC: 0.9895
Epoch 14/20
Train Loss: 0.1308, Train AUC: 0.9881
Val Loss: 0.1282, Val AUC: 0.9892
Epoch 15/20
Train Loss: 0.1283, Train AUC: 0.9886
Val Loss: 0.1289, Val AUC: 0.9888
EarlyStopping counter: 1 out of 5
Epoch 16/20
Train Loss: 0.1243, Train AUC: 0.9892
Val Loss: 0.1140, Val AUC: 0.9907
Epoch 17/20
Train Loss: 0.1230, Train AUC: 0.9895
Val Loss: 0.1493, Val AUC: 0.9864
EarlyStopping counter: 1 out of 5
Epoch 18/20
Train Loss: 0.1360, Train AUC: 0.9875
Val Loss: 0.1077, Val AUC: 0.9918
Epoch 19/20
Train Loss: 0.1183, Train AUC: 0.9904
Val Loss: 0.1003, Val AUC: 0.9926
Epoch 20/20
Train Loss: 0.1197, Train AUC: 0.9904
Val Loss: 0.1126, Val AUC: 0.9917
EarlyStopping counter: 1 out of 5
Training completed in 579.90 seconds
Evaluating model...
Test Loss: 0.0964, Test AUC: 0.9933
Inference completed in 4.33 seconds
Experiment completed successfully!

=== Experiment Completed ===
=== Results Summary ===
Best validation AUC: 0.9926 (Epoch 19)
Test AUC: 0.9933
Training time: 579.90 seconds
Inference time: 4.33 seconds


Result 2:

