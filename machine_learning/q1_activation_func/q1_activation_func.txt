How does the choice of activation function (e.g., ReLU, sigmoid, tanh) impact the convergence rate of a neural network on the MNIST dataset?
Model Configuration:
Architecture: Feedforward neural network with 2 hidden layers (128 neurons each), input layer (784 neurons for 28x28 MNIST images), output layer (10 neurons for digit classes).
Activation Functions to Test: ReLU, Sigmoid, Tanh (applied to hidden layers; softmax for output).
Optimizer: Stochastic Gradient Descent (SGD) with momentum (0.9).
Loss Function: Cross-entropy loss.
Weight Initialization: Xavier initialization for all layers.
Hyperparameters:
Learning Rate: 0.01 (fixed across experiments).
Batch Size: 64.
Epochs: 50 (with early stopping if loss < 0.05 for 5 consecutive epochs).
Momentum: 0.9.
Dataset:
MNIST: 60,000 training images, 10,000 test images (grayscale, 28x28 pixels).
Preprocessing: Normalize pixel values to [0, 1], flatten images into 784-dimensional vectors.
Evaluation Metric:
Primary: Training loss over epochs (plot loss curve).
Secondary: Test accuracy after training.
Additional Details:
Track epochs to reach training loss of 0.1.
Record computation time per epoch (in seconds) to assess efficiency.
Use a fixed random seed (e.g., 42) for reproducibility.

