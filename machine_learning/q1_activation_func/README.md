# Activation Function

## Question: How does the choice of activation function (e.g., ReLU, sigmoid, tanh) impact the convergence rate of a neural network on the MNIST dataset?
```bash
python3 -m curie.main -f benchmark/junior_ml_engineer_bench/q1_activation_func.txt --report
```

- Detailed question: `q1_diffusion_step.txt`
- **Estimated runtime**: 1h (Model training is time-consuming.)
- **Estimated cost**: $3 
- **Sample log fil**e: Available [here](mle_activation_func_20250326.log)
- **Sample report file**: Available [here](mle_activation_func_20250326.md)
- **Workspace (code, script, results ..)**: Available [here](workspace)